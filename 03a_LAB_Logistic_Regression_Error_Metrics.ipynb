{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "# Machine Learning Foundation\n",
    "\n",
    "## Course 3, Part a: Logistic Regression LAB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "We will be using the [Human Activity Recognition with Smartphones](https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones) database, which was built from the recordings of study participants performing activities of daily living (ADL) while carrying a smartphone with an embedded inertial sensors. The objective is to classify activities into one of the six activities (walking, walking upstairs, walking downstairs, sitting, standing, and laying) performed.\n",
    "\n",
    "For each record in the dataset it is provided: \n",
    "\n",
    "- Triaxial acceleration from the accelerometer (total acceleration) and the estimated body acceleration. \n",
    "- Triaxial Angular velocity from the gyroscope. \n",
    "- A 561-feature vector with time and frequency domain variables. \n",
    "- Its activity label. \n",
    "\n",
    "More information about the features is available on the website above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nites\\Desktop\\Jupyter Notebooks\\IBM Machine Learning Specilization\\Course 3\n"
     ]
    }
   ],
   "source": [
    "!cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "import os, seaborn as sns, pandas as pd, numpy as np\n",
    "os.chdir('./data/')\n",
    "from colorsetup import colors, palette\n",
    "sns.set_palette(palette)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "## Question 1\n",
    "\n",
    "Import the data and do the following:\n",
    "\n",
    "* Examine the data types--there are many columns, so it might be wise to use value counts\n",
    "* Determine if the floating point values need to be scaled\n",
    "* Determine the breakdown of each activity\n",
    "* Encode the activity label as an integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "filepath = 'Human_Activity_Recognition_Using_Smartphones_Data.csv'\n",
    "data = pd.read_csv(filepath, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "The data columns are all floats except for the activity label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tBodyAcc-mean()-X</th>\n",
       "      <th>tBodyAcc-mean()-Y</th>\n",
       "      <th>tBodyAcc-mean()-Z</th>\n",
       "      <th>tBodyAcc-std()-X</th>\n",
       "      <th>tBodyAcc-std()-Y</th>\n",
       "      <th>tBodyAcc-std()-Z</th>\n",
       "      <th>tBodyAcc-mad()-X</th>\n",
       "      <th>tBodyAcc-mad()-Y</th>\n",
       "      <th>tBodyAcc-mad()-Z</th>\n",
       "      <th>tBodyAcc-max()-X</th>\n",
       "      <th>...</th>\n",
       "      <th>fBodyBodyGyroJerkMag-skewness()</th>\n",
       "      <th>fBodyBodyGyroJerkMag-kurtosis()</th>\n",
       "      <th>angle(tBodyAccMean,gravity)</th>\n",
       "      <th>angle(tBodyAccJerkMean),gravityMean)</th>\n",
       "      <th>angle(tBodyGyroMean,gravityMean)</th>\n",
       "      <th>angle(tBodyGyroJerkMean,gravityMean)</th>\n",
       "      <th>angle(X,gravityMean)</th>\n",
       "      <th>angle(Y,gravityMean)</th>\n",
       "      <th>angle(Z,gravityMean)</th>\n",
       "      <th>Activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.288585</td>\n",
       "      <td>-0.020294</td>\n",
       "      <td>-0.132905</td>\n",
       "      <td>-0.995279</td>\n",
       "      <td>-0.983111</td>\n",
       "      <td>-0.913526</td>\n",
       "      <td>-0.995112</td>\n",
       "      <td>-0.983185</td>\n",
       "      <td>-0.923527</td>\n",
       "      <td>-0.934724</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.298676</td>\n",
       "      <td>-0.710304</td>\n",
       "      <td>-0.112754</td>\n",
       "      <td>0.030400</td>\n",
       "      <td>-0.464761</td>\n",
       "      <td>-0.018446</td>\n",
       "      <td>-0.841247</td>\n",
       "      <td>0.179941</td>\n",
       "      <td>-0.058627</td>\n",
       "      <td>STANDING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.278419</td>\n",
       "      <td>-0.016411</td>\n",
       "      <td>-0.123520</td>\n",
       "      <td>-0.998245</td>\n",
       "      <td>-0.975300</td>\n",
       "      <td>-0.960322</td>\n",
       "      <td>-0.998807</td>\n",
       "      <td>-0.974914</td>\n",
       "      <td>-0.957686</td>\n",
       "      <td>-0.943068</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.595051</td>\n",
       "      <td>-0.861499</td>\n",
       "      <td>0.053477</td>\n",
       "      <td>-0.007435</td>\n",
       "      <td>-0.732626</td>\n",
       "      <td>0.703511</td>\n",
       "      <td>-0.844788</td>\n",
       "      <td>0.180289</td>\n",
       "      <td>-0.054317</td>\n",
       "      <td>STANDING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.279653</td>\n",
       "      <td>-0.019467</td>\n",
       "      <td>-0.113462</td>\n",
       "      <td>-0.995380</td>\n",
       "      <td>-0.967187</td>\n",
       "      <td>-0.978944</td>\n",
       "      <td>-0.996520</td>\n",
       "      <td>-0.963668</td>\n",
       "      <td>-0.977469</td>\n",
       "      <td>-0.938692</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.390748</td>\n",
       "      <td>-0.760104</td>\n",
       "      <td>-0.118559</td>\n",
       "      <td>0.177899</td>\n",
       "      <td>0.100699</td>\n",
       "      <td>0.808529</td>\n",
       "      <td>-0.848933</td>\n",
       "      <td>0.180637</td>\n",
       "      <td>-0.049118</td>\n",
       "      <td>STANDING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.279174</td>\n",
       "      <td>-0.026201</td>\n",
       "      <td>-0.123283</td>\n",
       "      <td>-0.996091</td>\n",
       "      <td>-0.983403</td>\n",
       "      <td>-0.990675</td>\n",
       "      <td>-0.997099</td>\n",
       "      <td>-0.982750</td>\n",
       "      <td>-0.989302</td>\n",
       "      <td>-0.938692</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.117290</td>\n",
       "      <td>-0.482845</td>\n",
       "      <td>-0.036788</td>\n",
       "      <td>-0.012892</td>\n",
       "      <td>0.640011</td>\n",
       "      <td>-0.485366</td>\n",
       "      <td>-0.848649</td>\n",
       "      <td>0.181935</td>\n",
       "      <td>-0.047663</td>\n",
       "      <td>STANDING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.276629</td>\n",
       "      <td>-0.016570</td>\n",
       "      <td>-0.115362</td>\n",
       "      <td>-0.998139</td>\n",
       "      <td>-0.980817</td>\n",
       "      <td>-0.990482</td>\n",
       "      <td>-0.998321</td>\n",
       "      <td>-0.979672</td>\n",
       "      <td>-0.990441</td>\n",
       "      <td>-0.942469</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.351471</td>\n",
       "      <td>-0.699205</td>\n",
       "      <td>0.123320</td>\n",
       "      <td>0.122542</td>\n",
       "      <td>0.693578</td>\n",
       "      <td>-0.615971</td>\n",
       "      <td>-0.847865</td>\n",
       "      <td>0.185151</td>\n",
       "      <td>-0.043892</td>\n",
       "      <td>STANDING</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 562 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   tBodyAcc-mean()-X  tBodyAcc-mean()-Y  tBodyAcc-mean()-Z  tBodyAcc-std()-X  \\\n",
       "0           0.288585          -0.020294          -0.132905         -0.995279   \n",
       "1           0.278419          -0.016411          -0.123520         -0.998245   \n",
       "2           0.279653          -0.019467          -0.113462         -0.995380   \n",
       "3           0.279174          -0.026201          -0.123283         -0.996091   \n",
       "4           0.276629          -0.016570          -0.115362         -0.998139   \n",
       "\n",
       "   tBodyAcc-std()-Y  tBodyAcc-std()-Z  tBodyAcc-mad()-X  tBodyAcc-mad()-Y  \\\n",
       "0         -0.983111         -0.913526         -0.995112         -0.983185   \n",
       "1         -0.975300         -0.960322         -0.998807         -0.974914   \n",
       "2         -0.967187         -0.978944         -0.996520         -0.963668   \n",
       "3         -0.983403         -0.990675         -0.997099         -0.982750   \n",
       "4         -0.980817         -0.990482         -0.998321         -0.979672   \n",
       "\n",
       "   tBodyAcc-mad()-Z  tBodyAcc-max()-X  ...  fBodyBodyGyroJerkMag-skewness()  \\\n",
       "0         -0.923527         -0.934724  ...                        -0.298676   \n",
       "1         -0.957686         -0.943068  ...                        -0.595051   \n",
       "2         -0.977469         -0.938692  ...                        -0.390748   \n",
       "3         -0.989302         -0.938692  ...                        -0.117290   \n",
       "4         -0.990441         -0.942469  ...                        -0.351471   \n",
       "\n",
       "   fBodyBodyGyroJerkMag-kurtosis()  angle(tBodyAccMean,gravity)  \\\n",
       "0                        -0.710304                    -0.112754   \n",
       "1                        -0.861499                     0.053477   \n",
       "2                        -0.760104                    -0.118559   \n",
       "3                        -0.482845                    -0.036788   \n",
       "4                        -0.699205                     0.123320   \n",
       "\n",
       "   angle(tBodyAccJerkMean),gravityMean)  angle(tBodyGyroMean,gravityMean)  \\\n",
       "0                              0.030400                         -0.464761   \n",
       "1                             -0.007435                         -0.732626   \n",
       "2                              0.177899                          0.100699   \n",
       "3                             -0.012892                          0.640011   \n",
       "4                              0.122542                          0.693578   \n",
       "\n",
       "   angle(tBodyGyroJerkMean,gravityMean)  angle(X,gravityMean)  \\\n",
       "0                             -0.018446             -0.841247   \n",
       "1                              0.703511             -0.844788   \n",
       "2                              0.808529             -0.848933   \n",
       "3                             -0.485366             -0.848649   \n",
       "4                             -0.615971             -0.847865   \n",
       "\n",
       "   angle(Y,gravityMean)  angle(Z,gravityMean)  Activity  \n",
       "0              0.179941             -0.058627  STANDING  \n",
       "1              0.180289             -0.054317  STANDING  \n",
       "2              0.180637             -0.049118  STANDING  \n",
       "3              0.181935             -0.047663  STANDING  \n",
       "4              0.185151             -0.043892  STANDING  \n",
       "\n",
       "[5 rows x 562 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float64    561\n",
       "object       1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "angle(tBodyGyroJerkMean,gravityMean)    float64\n",
       "angle(X,gravityMean)                    float64\n",
       "angle(Y,gravityMean)                    float64\n",
       "angle(Z,gravityMean)                    float64\n",
       "Activity                                 object\n",
       "dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "The data are all scaled from -1 (minimum) to 1.0 (maximum)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0    561\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[:, :-1].min().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    561\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[:, :-1].max().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the breakdown of activities--they are relatively balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LAYING                1944\n",
       "STANDING              1906\n",
       "SITTING               1777\n",
       "WALKING               1722\n",
       "WALKING_UPSTAIRS      1544\n",
       "WALKING_DOWNSTAIRS    1406\n",
       "Name: Activity, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Activity.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit learn classifiers won't accept a sparse matrix for the prediction column. Thus, either `LabelEncoder` needs to be used to convert the activity labels to integers, or if `DictVectorizer` is used, the resulting matrix must be converted to a non-sparse array.  \n",
    "Use `LabelEncoder` to fit_transform the \"Activity\" column, and look at 5 random values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6504     1\n",
       "4301     0\n",
       "926      3\n",
       "10148    1\n",
       "1101     3\n",
       "Name: Activity, dtype: int32"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "data['Activity'] = le.fit_transform(data.Activity)\n",
    "data['Activity'].sample(5)\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "* Calculate the correlations between the dependent variables.\n",
    "* Create a histogram of the correlation values\n",
    "* Identify those that are most correlated (either positively or negatively)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "# Calculate the correlation values\n",
    "feature_cols = data.columns[:-1]\n",
    "corr_values = data[feature_cols].corr()\n",
    "\n",
    "# Simplify by emptying all the data below the diagonal\n",
    "tril_index = np.tril_indices_from(corr_values)\n",
    "\n",
    "# Make the unused values NaNs\n",
    "for coord in zip(*tril_index):\n",
    "    corr_values.iloc[coord[0], coord[1]] = np.NaN\n",
    "    \n",
    "# Stack the data and convert to a data frame\n",
    "corr_values = (corr_values\n",
    "               .stack()\n",
    "               .to_frame()\n",
    "               .reset_index()\n",
    "               .rename(columns={'level_0':'feature1',\n",
    "                                'level_1':'feature2',\n",
    "                                0:'correlation'}))\n",
    "\n",
    "# Get the absolute values for sorting\n",
    "corr_values['abs_correlation'] = corr_values.correlation.abs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A histogram of the absolute value correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sns.set_context('talk')\n",
    "sns.set_style('white')\n",
    "\n",
    "ax = corr_values.abs_correlation.hist(bins=50, figsize=(12, 8))\n",
    "ax.set(xlabel='Absolute Correlation', ylabel='Frequency');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# The most highly correlated values\n",
    "corr_values.sort_values('correlation', ascending=False).query('abs_correlation>0.8')\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "* Split the data into train and test data sets. This can be done using any method, but consider using Scikit-learn's `StratifiedShuffleSplit` to maintain the same ratio of predictor classes.\n",
    "* Regardless of methods used to split the data, compare the ratio of classes in both the train and test splits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# Get the split indexes\n",
    "strat_shuf_split = StratifiedShuffleSplit(n_splits=1, \n",
    "                                          test_size=0.3, \n",
    "                                          random_state=42)\n",
    "\n",
    "train_idx, test_idx = next(strat_shuf_split.split(data[feature_cols], data.Activity))\n",
    "\n",
    "# Create the dataframes\n",
    "X_train = data.loc[train_idx, feature_cols]\n",
    "y_train = data.loc[train_idx, 'Activity']\n",
    "\n",
    "X_test  = data.loc[test_idx, feature_cols]\n",
    "y_test  = data.loc[test_idx, 'Activity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "y_test.value_counts(normalize=True)\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "* Fit a logistic regression model without any regularization using all of the features. Be sure to read the documentation about fitting a multi-class model so you understand the coefficient output. Store the model.\n",
    "* Using cross validation to determine the hyperparameters, fit models using L1, and L2 regularization. Store each of these models as well. Note the limitations on multi-class models, solvers, and regularizations. The regularized models, in particular the L1 model, will probably take a while to fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Standard logistic regression\n",
    "lr = LogisticRegression(solver='liblinear').fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "# L1 regularized logistic regression\n",
    "lr_l1 = LogisticRegressionCV(Cs=10, cv=4, penalty='l1', solver='liblinear').fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2 regularized logistic regression\n",
    "lr_l2 = LogisticRegressionCV(Cs=10, cv=4, penalty='l2', solver='liblinear').fit(X_train, y_train)\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "\n",
    "Compare the magnitudes of the coefficients for each of the models. If one-vs-rest fitting was used, each set of coefficients can be plotted separately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "# Combine all the coefficients into a dataframe\n",
    "coefficients = list()\n",
    "\n",
    "coeff_labels = ['lr', 'l1', 'l2']\n",
    "coeff_models = [lr, lr_l1, lr_l2]\n",
    "\n",
    "for lab,mod in zip(coeff_labels, coeff_models):\n",
    "    coeffs = mod.coef_\n",
    "    coeff_label = pd.MultiIndex(levels=[[lab], [0,1,2,3,4,5]], \n",
    "                                 codes=[[0,0,0,0,0,0], [0,1,2,3,4,5]])\n",
    "    coefficients.append(pd.DataFrame(coeffs.T, columns=coeff_label))\n",
    "\n",
    "coefficients = pd.concat(coefficients, axis=1)\n",
    "\n",
    "coefficients.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare six separate plots for each of the multi-class coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig, axList = plt.subplots(nrows=3, ncols=2)\n",
    "axList = axList.flatten()\n",
    "fig.set_size_inches(10,10)\n",
    "\n",
    "for ax in enumerate(axList):\n",
    "    loc = ax[0]\n",
    "    ax = ax[1]\n",
    "    \n",
    "    data = coefficients.xs(loc, level=1, axis=1)\n",
    "    data.plot(marker='o', ls='', ms=2.0, ax=ax, legend=False)\n",
    "    \n",
    "    if ax is axList[0]:\n",
    "        ax.legend(loc=4)\n",
    "        \n",
    "    ax.set(title='Coefficient Set '+str(loc))\n",
    "\n",
    "plt.tight_layout()\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "\n",
    "* Predict and store the class for each model.\n",
    "* Store the probability for the predicted class for each model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "# Predict the class and the probability for each\n",
    "y_pred = list()\n",
    "y_prob = list()\n",
    "\n",
    "coeff_labels = ['lr', 'l1', 'l2']\n",
    "coeff_models = [lr, lr_l1, lr_l2]\n",
    "\n",
    "for lab,mod in zip(coeff_labels, coeff_models):\n",
    "    y_pred.append(pd.Series(mod.predict(X_test), name=lab))\n",
    "    y_prob.append(pd.Series(mod.predict_proba(X_test).max(axis=1), name=lab))\n",
    "    \n",
    "y_pred = pd.concat(y_pred, axis=1)\n",
    "y_prob = pd.concat(y_prob, axis=1)\n",
    "\n",
    "y_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "y_prob.head()\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7\n",
    "\n",
    "For each model, calculate the following error metrics: \n",
    "\n",
    "* Accuracy\n",
    "* Precision\n",
    "* Recall\n",
    "* F-score\n",
    "* Confusion Matrix\n",
    "\n",
    "Decide how to combine the multi-class metrics into a single value for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "metrics = list()\n",
    "cm = dict()\n",
    "\n",
    "for lab in coeff_labels:\n",
    "\n",
    "    # Preciision, recall, f-score from the multi-class support function\n",
    "    precision, recall, fscore, _ = score(y_test, y_pred[lab], average='weighted')\n",
    "    \n",
    "    # The usual way to calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred[lab])\n",
    "    \n",
    "    # ROC-AUC scores can be calculated by binarizing the data\n",
    "    auc = roc_auc_score(label_binarize(y_test, classes=[0,1,2,3,4,5]),\n",
    "              label_binarize(y_pred[lab], classes=[0,1,2,3,4,5]), \n",
    "              average='weighted')\n",
    "    \n",
    "    # Last, the confusion matrix\n",
    "    cm[lab] = confusion_matrix(y_test, y_pred[lab])\n",
    "    \n",
    "    metrics.append(pd.Series({'precision':precision, 'recall':recall, \n",
    "                              'fscore':fscore, 'accuracy':accuracy,\n",
    "                              'auc':auc}, \n",
    "                             name=lab))\n",
    "\n",
    "metrics = pd.concat(metrics, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "metrics\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8\n",
    "\n",
    "Display or plot the confusion matrix for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "fig, axList = plt.subplots(nrows=2, ncols=2)\n",
    "axList = axList.flatten()\n",
    "fig.set_size_inches(12, 10)\n",
    "\n",
    "axList[-1].axis('off')\n",
    "\n",
    "for ax,lab in zip(axList[:-1], coeff_labels):\n",
    "    sns.heatmap(cm[lab], ax=ax, annot=True, fmt='d', cmap=colors);\n",
    "    ax.set(title=lab);\n",
    "    \n",
    "plt.tight_layout()\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Machine Learning Foundation (C) 2020 IBM Corporation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
